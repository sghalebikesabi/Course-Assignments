{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "fv0U92HJdv03",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Assignment 2"
      ]
    },
    {
      "metadata": {
        "id": "Q3fOYicSdxvF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## P1 (max points 25)"
      ]
    },
    {
      "metadata": {
        "id": "k-DBbhgyd5mm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Consider a two-player game that proceeds in successive rounds where in each round one player wins and the other loses.  The winner of the game is the player who first wins  <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;d\" title=\"\\Large x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}\" /> rounds more than the opponent, where <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;d{\\geq}1\" title=\"\\Large x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}\" /> is a parameter. \n",
        "\n",
        "Suppose we model this game so that we consider one of the players. In each round, this player has two available actions, either to invest high or low effort. \n",
        "\n",
        "If the player invests high effort in a round, she wins this round with probability  <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;p_H\" title=\"a\" /> , otherwise, she wins this round with probability <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;p_L\" title=\"a\" /> .\n",
        "\n",
        "<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;p_L\" title=\"a\" /> and <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;p_H\" title=\"a\" />  are parameters such that <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;0<p_L<p_H<1\" title=\"x\" /> . \n",
        "\n",
        "By investing high effort in a round, the player incurs a cost of value <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;c_H\" title=\"x\" />, and otherwise, a cost of value <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;c_L\" title=\"x\" />, in this round.\n",
        "\n",
        "<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;c_L\" title=\"x\" /> and <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;c_H\" title=\"x\" /> are parameters such that <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;0<c_L<c_H\" title=\"x\" />.\n",
        "\n",
        "The player receives a prize of value <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;R\" title=\"x\" /> if winning the game. \n",
        "\n",
        "Assume the following values for parameters:\n",
        "* <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;d=3\" title=\"x\" />\n",
        "* <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;p_H=0.55\" title=\"x\" />\n",
        "* <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;p_L=0.45\" title=\"x\" />\n",
        "* <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;c_H=50\" title=\"x\" />\n",
        "* <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;c_L=10\" title=\"x\" />\n",
        "* <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;R=1000\" title=\"x\" />"
      ]
    },
    {
      "metadata": {
        "id": "X03vLDpid9SF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Questions:\n",
        "\n",
        "1. **[max points 5]** Compute numerically the value function, by assuming perfect knowledge of the environment, for each deterministic policy and rank these policies with respect to the value at the initial state. What is the optimal deterministic policy? Discuss the results. "
      ]
    },
    {
      "metadata": {
        "id": "BQcHki0YwENT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Imagine we consider player $X$. Let $s_Y$ be the score of $Y$, $s_X$ the score of $X$ and $s=s_X-s_Y$. There are thus five different states for $X$ after which $X$ has to take actions, i.e. $s\\in\\{-2,-1,0,1,2\\}$. In each of these states two actions are possible: going in low effort or high effort. For $s=-3$ $X$ loses and wins for $s=3$.  Since $s=-3$ and $s=3$ are the final states, the value function is $0$ for those."
      ]
    },
    {
      "metadata": {
        "id": "pxZc-7Qhnp1a",
        "colab_type": "code",
        "outputId": "60968cd3-c2a2-475f-ed2b-49c7aa18592e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1216
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "# parameter setting\n",
        "d = 3\n",
        "pH = 0.55\n",
        "pL = 0.45\n",
        "cH = 50\n",
        "cL = 10\n",
        "R = 1000\n",
        "tol = math.exp(-6)\n",
        "\n",
        "# function value: calculate the value function for each policy\n",
        "# input: policy - vector of 5 elements, either \"high\" or \"low\"\n",
        "# output: array of length 7 with values for states -3 to 3\n",
        "def value(policy):\n",
        "  epsilon = 10\n",
        "  states = np.zeros(7)\n",
        "  states[0] = 0\n",
        "  states[6] = R\n",
        "  while epsilon > tol:\n",
        "    old_states = list(states)\n",
        "    for s in range(1,6):\n",
        "      p = (policy[s - 1]==\"high\") * pH + (policy[s - 1]==\"low\") * pL\n",
        "      c = (policy[s - 1]==\"high\") * cH + (policy[s - 1]==\"low\") * cL\n",
        "      states[s] = p * states[s+1] + (1 - p) * states[s-1] - c\n",
        "    epsilon = np.abs(states - old_states).max()\n",
        "  states[6] = 0\n",
        "  return(states)\n",
        "\n",
        "# save deterministic_policies\n",
        "x = [\"low\", \"high\"]\n",
        "deterministic_policies = [p for p in itertools.product(x, repeat=5)]\n",
        "\n",
        "# save information in form of dataframe\n",
        "i = 0\n",
        "estimated_value = pd.DataFrame({\"Deterministic policy\": str(np.zeros(2^5)), \"Rank\": np.zeros(2^5), \"Value s=-3\": np.zeros(2^5), \"Value s=-2\": np.zeros(2^5), \"Value s=-1\": np.zeros(2^5), \"Value s= 0\": np.zeros(2^5), \"Value s= 1\": np.zeros(2^5), \"Value s= 2\": np.zeros(2^5), \"Value s= 3\": np.zeros(2^5)})\n",
        "for current_policy in deterministic_policies:\n",
        "  values_policy = value(current_policy)\n",
        "  estimated_value.at[i,\"Deterministic policy\"] = str(current_policy)\n",
        "  estimated_value.at[i,\"Value s=-3\"] = round(values_policy[0],6)\n",
        "  estimated_value.at[i,\"Value s=-2\"] = round(values_policy[1],6)\n",
        "  estimated_value.at[i,\"Value s=-1\"] = round(values_policy[2],6)\n",
        "  estimated_value.at[i,\"Value s= 0\"] = round(values_policy[3],6)\n",
        "  estimated_value.at[i,\"Value s= 1\"] = round(values_policy[4],6)\n",
        "  estimated_value.at[i,\"Value s= 2\"] = round(values_policy[5],6)\n",
        "  estimated_value.at[i,\"Value s= 3\"] = round(values_policy[6],6)\n",
        "  i += 1\n",
        "\n",
        "# rank poilicies wrt the initial state  \n",
        "array = estimated_value.loc[:,\"Value s= 0\"]\n",
        "temp = array.argsort()[::-1]\n",
        "ranks = np.empty_like(temp)\n",
        "ranks[temp] = np.arange(len(array))\n",
        "estimated_value[\"Rank\"] = ranks\n",
        "\n",
        "# print result\n",
        "print(estimated_value)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                        Deterministic policy  Rank  Value s=-3  Value s=-2  \\\n",
            "0        ('low', 'low', 'low', 'low', 'low')     3         0.0   52.366106   \n",
            "1       ('low', 'low', 'low', 'low', 'high')     1         0.0   55.313122   \n",
            "2       ('low', 'low', 'low', 'high', 'low')     2         0.0   53.196826   \n",
            "3      ('low', 'low', 'low', 'high', 'high')     0         0.0   56.521498   \n",
            "4       ('low', 'low', 'high', 'low', 'low')    12         0.0   44.659670   \n",
            "5      ('low', 'low', 'high', 'low', 'high')     5         0.0   48.852513   \n",
            "6      ('low', 'low', 'high', 'high', 'low')     8         0.0   46.653802   \n",
            "7     ('low', 'low', 'high', 'high', 'high')     4         0.0   51.139070   \n",
            "8       ('low', 'high', 'low', 'low', 'low')    18         0.0   28.206202   \n",
            "9      ('low', 'high', 'low', 'low', 'high')    10         0.0   32.974294   \n",
            "10     ('low', 'high', 'low', 'high', 'low')    15         0.0   30.905022   \n",
            "11    ('low', 'high', 'low', 'high', 'high')     7         0.0   36.008615   \n",
            "12     ('low', 'high', 'high', 'low', 'low')    27         0.0   20.367709   \n",
            "13    ('low', 'high', 'high', 'low', 'high')    20         0.0   26.766992   \n",
            "14    ('low', 'high', 'high', 'high', 'low')    22         0.0   24.725198   \n",
            "15   ('low', 'high', 'high', 'high', 'high')    14         0.0   31.248294   \n",
            "16      ('high', 'low', 'low', 'low', 'low')    16         0.0    2.168671   \n",
            "17     ('high', 'low', 'low', 'low', 'high')     9         0.0    7.061096   \n",
            "18     ('high', 'low', 'low', 'high', 'low')    11         0.0    4.726575   \n",
            "19    ('high', 'low', 'low', 'high', 'high')     6         0.0   10.048253   \n",
            "20     ('high', 'low', 'high', 'low', 'low')    24         0.0   -6.671608   \n",
            "21    ('high', 'low', 'high', 'low', 'high')    17         0.0   -0.004752   \n",
            "22    ('high', 'low', 'high', 'high', 'low')    21         0.0   -2.370739   \n",
            "23   ('high', 'low', 'high', 'high', 'high')    13         0.0    4.535879   \n",
            "24     ('high', 'high', 'low', 'low', 'low')    29         0.0  -29.148095   \n",
            "25    ('high', 'high', 'low', 'low', 'high')    25         0.0  -21.487099   \n",
            "26    ('high', 'high', 'low', 'high', 'low')    26         0.0  -23.453773   \n",
            "27   ('high', 'high', 'low', 'high', 'high')    19         0.0  -15.531273   \n",
            "28    ('high', 'high', 'high', 'low', 'low')    31         0.0  -37.188586   \n",
            "29   ('high', 'high', 'high', 'low', 'high')    28         0.0  -27.331283   \n",
            "30   ('high', 'high', 'high', 'high', 'low')    30         0.0  -29.217583   \n",
            "31  ('high', 'high', 'high', 'high', 'high')    23         0.0  -19.474046   \n",
            "\n",
            "    Value s=-1  Value s= 0  Value s= 1  Value s= 2  Value s= 3  \n",
            "0   138.593443  266.206940  444.403118  684.421715         0.0  \n",
            "1   145.142576  277.158778  460.735991  707.331196         0.0  \n",
            "2   140.439408  269.293586  449.005899  686.953245         0.0  \n",
            "3   147.827590  281.648188  467.430342  710.343654         0.0  \n",
            "4   121.467899  237.569051  423.471802  672.909491         0.0  \n",
            "5   130.785667  253.150863  444.178804  699.880462         0.0  \n",
            "6   125.899361  244.979434  433.319342  678.325638         0.0  \n",
            "7   135.866888  261.647618  455.469812  704.961415         0.0  \n",
            "8    84.904949  222.206288  412.243169  666.733743         0.0  \n",
            "9    95.500500  237.569466  433.433092  695.044891         0.0  \n",
            "10   90.902288  230.902288  424.237305  673.330518         0.0  \n",
            "11  102.243260  247.346137  446.917551  701.112898         0.0  \n",
            "12   67.485645  196.947708  393.781865  656.580026         0.0  \n",
            "13   81.706718  217.568787  419.639478  688.837765         0.0  \n",
            "14   77.169577  210.990175  411.390705  666.264888         0.0  \n",
            "15   91.665209  232.008520  437.745463  696.985458         0.0  \n",
            "16   94.854042  230.360377  418.203067  670.011687         0.0  \n",
            "17  103.749356  244.148027  437.970062  697.086528         0.0  \n",
            "18   99.504937  237.569454  428.538756  675.696316         0.0  \n",
            "19  109.180607  252.566459  450.039325  702.517696         0.0  \n",
            "20   78.780907  205.447210  399.994076  659.996742         0.0  \n",
            "21   90.902590  224.235923  424.237532  690.906889         0.0  \n",
            "22   86.600973  217.568714  415.634974  668.599236         0.0  \n",
            "23   99.158596  237.033101  440.750175  698.337579         0.0  \n",
            "24   37.914664  183.695395  384.095748  651.252662         0.0  \n",
            "25   51.843903  202.753274  409.421908  684.239859         0.0  \n",
            "26   48.268106  197.860886  402.920313  661.606172         0.0  \n",
            "27   62.672774  217.569164  429.110650  693.099792         0.0  \n",
            "28   23.295820  163.694491  369.477087  643.212398         0.0  \n",
            "29   41.217914  188.214686  399.395335  679.727901         0.0  \n",
            "30   37.788242  183.522215  393.669626  656.518294         0.0  \n",
            "31   55.504058  207.760961  423.245293  690.460382         0.0  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MT1VIX3FD3nb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Deterministic policies are defined such that the policy $(a_0,...,a_4)$ corresponds to taking action $a_i$ in state $s_{i}=-2+i$ for $i\\in\\{0,...,4\\}$, where the string `high` means putting in high effort, and `low` means spending low effort."
      ]
    },
    {
      "metadata": {
        "id": "Y5o-JAO1rBZ7",
        "colab_type": "code",
        "outputId": "ed6b249d-13a9-45e1-ca0e-8ce117016aa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "cell_type": "code",
      "source": [
        "# print descriptive statistics for value functions\n",
        "estimated_value.drop(['Rank'], axis=1).describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Value s= 0</th>\n",
              "      <th>Value s= 1</th>\n",
              "      <th>Value s= 2</th>\n",
              "      <th>Value s= 3</th>\n",
              "      <th>Value s=-1</th>\n",
              "      <th>Value s=-2</th>\n",
              "      <th>Value s=-3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>32.000000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>32.0</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>32.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>227.544062</td>\n",
              "      <td>424.507500</td>\n",
              "      <td>681.667500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>90.595313</td>\n",
              "      <td>14.271563</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>28.894324</td>\n",
              "      <td>23.009805</td>\n",
              "      <td>17.946851</td>\n",
              "      <td>0.0</td>\n",
              "      <td>34.345156</td>\n",
              "      <td>29.540341</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>163.700000</td>\n",
              "      <td>369.480000</td>\n",
              "      <td>643.220000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.310000</td>\n",
              "      <td>-37.180000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>207.190000</td>\n",
              "      <td>410.907500</td>\n",
              "      <td>668.135000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>66.287500</td>\n",
              "      <td>-8.885000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>230.640000</td>\n",
              "      <td>424.240000</td>\n",
              "      <td>684.330000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>91.290000</td>\n",
              "      <td>15.210000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>245.580000</td>\n",
              "      <td>441.607500</td>\n",
              "      <td>697.015000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>112.260000</td>\n",
              "      <td>38.172500</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>281.650000</td>\n",
              "      <td>467.430000</td>\n",
              "      <td>710.350000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>147.830000</td>\n",
              "      <td>56.520000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Value s= 0  Value s= 1  Value s= 2  Value s= 3  Value s=-1  Value s=-2  \\\n",
              "count   32.000000   32.000000   32.000000        32.0   32.000000   32.000000   \n",
              "mean   227.544062  424.507500  681.667500         0.0   90.595313   14.271563   \n",
              "std     28.894324   23.009805   17.946851         0.0   34.345156   29.540341   \n",
              "min    163.700000  369.480000  643.220000         0.0   23.310000  -37.180000   \n",
              "25%    207.190000  410.907500  668.135000         0.0   66.287500   -8.885000   \n",
              "50%    230.640000  424.240000  684.330000         0.0   91.290000   15.210000   \n",
              "75%    245.580000  441.607500  697.015000         0.0  112.260000   38.172500   \n",
              "max    281.650000  467.430000  710.350000         0.0  147.830000   56.520000   \n",
              "\n",
              "       Value s=-3  \n",
              "count        32.0  \n",
              "mean          0.0  \n",
              "std           0.0  \n",
              "min           0.0  \n",
              "25%           0.0  \n",
              "50%           0.0  \n",
              "75%           0.0  \n",
              "max           0.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "wJn4wQJpoCqv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The optimal policy is putting in low effort in states -2, -1 and 0 and high effort otherwise. This optimal policy does not only have the highest value function with respect to the intial state, but also from all other states which results from the use of dynamic programming with the Markov property.\n",
        "\n",
        "In general,  we can see that policies that specify high effort for higher states have higher rankings, while putting in high effort in negative valued states leads to lower rankings (worst policy wrt initial state puts in high effort only for s<1).\n",
        "\n",
        "The value function for the policies is the highest for s=2 (681.66 on average) and decreases with the score difference such that state s=-2 has the smallest value function (14.27 on average) leaving the terminal states aside. This is as expected since state 2 is the only state with a positive incremental reward."
      ]
    },
    {
      "metadata": {
        "id": "oqr9gdBaeBHm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2. **[max points 10]** Estimate the action-value function by using the off-policy Monte Carlo estimation method for a threshold policy <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\pi\" title=\"x\" />\n",
        "such that <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\pi(s)=high\" title=\"x\" /> whenever <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;s<=s^*\" title=\"x\" /> and <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\pi(s)=low\" title=\"x\" />  otherwise, for given threshold value <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;s^*=1\" title=\"x\" /> where the behaviour policy, in each state, selects either action equiprobably. Show action values. Can you conclude from the obtained results whether the target policy is optimal? Discuss your answer."
      ]
    },
    {
      "metadata": {
        "id": "vw6RVRB-y6tJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# parameter setting\n",
        "d = 3\n",
        "pH = 0.55\n",
        "pL = 0.45\n",
        "cH = 50\n",
        "cL = 10\n",
        "R = 1000\n",
        "num_episodes = 500000\n",
        "pi = [1, 1, 1, 1, 0]\n",
        "\n",
        "# function run\n",
        "# does as task specifies, prints Q\n",
        "def run():\n",
        "  Q = np.random.rand(5,2)\n",
        "  C = np.zeros((5,2))\n",
        "\n",
        "  for i in range(num_episodes):\n",
        "\n",
        "    if i%100000==0:\n",
        "      print(\"\\n Episode\", i )\n",
        "\n",
        "    # run once with behavior policy\n",
        "    states = []\n",
        "    rewards = []\n",
        "    actions = []\n",
        "    s = 0\n",
        "    while(abs(s) < 3):\n",
        "      states.append(s)\n",
        "      high_effort = (np.random.uniform() < 0.5) * 1\n",
        "      actions.append(high_effort)\n",
        "      p = high_effort * pH + (1 - high_effort) * pL\n",
        "      Xwins = (np.random.uniform() < p)\n",
        "      s = s + Xwins - (1 - Xwins)\n",
        "      c = high_effort * cH + (1 - high_effort) * cL\n",
        "      rewards.append(-c)\n",
        "    if s==3:\n",
        "      rewards[-1] += R\n",
        "\n",
        "    # update Q\n",
        "    G = 0\n",
        "    W = 1\n",
        "    for t in reversed(range(len(states))):\n",
        "      s = states[t]\n",
        "      a = actions[t]\n",
        "      G += rewards[t]\n",
        "      C[s+2,a] += W\n",
        "      Q[s+2,a] = Q[s+2,a] + W/C[s+2,a] * (G-Q[s+2,a])\n",
        "      W *= (pi[s+2]==a)/0.5\n",
        "      if W==0:\n",
        "        break\n",
        "  # print results\n",
        "  print(\"\\n Q:\")\n",
        "  print(np.vstack(([0,0],Q,[0,0])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lCZeR889N5Ww",
        "colab_type": "code",
        "outputId": "605cd53f-b1b9-4f5c-cb0d-3978626e8eba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "cell_type": "code",
      "source": [
        "run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Episode 0\n",
            "\n",
            " Episode 100000\n",
            "\n",
            " Episode 200000\n",
            "\n",
            " Episode 300000\n",
            "\n",
            " Episode 400000\n",
            "\n",
            " Episode 500000\n",
            "\n",
            " Q:\n",
            "[[  0.           0.        ]\n",
            " [ 28.73035711 -30.64956996]\n",
            " [137.81446908  87.23636183]\n",
            " [317.49818106 233.13817539]\n",
            " [522.96030684 516.1348077 ]\n",
            " [741.09704004 772.01528426]\n",
            " [  0.           0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5Dci7y-URROG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "From now on we show results for all seven states for a score difference from -3 to 3. `Q[i,j]` (row i for $i\\in\\{0,..,6\\}$, column j for $j\\in\\{0,1\\}$) is the value of that state with score difference `i-3` and action `high effort` for $j=1$ and action `low effort` for $j=0$. "
      ]
    },
    {
      "metadata": {
        "id": "7hSLbF471Ogm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The value function for the terminal states is 0.\n",
        "\n",
        "The action-state pairs for actions not taken under the policy tell us the value of deviating only once from that policy in off-policy Monte Carlo. Thus if the target policy was optimal, it would choose the action with the highest value for each state correspoing to the state value function matrix 'Q' displayed.  Since 'Q'  however suggests putting in low effort in states -2,-1,0 and 1 and high effort otherwise, the target policy is apparently not optimal.\n",
        "\n",
        "However, running the code several times, we notice the volatility in the state action values. Apparently, 500,000 episodes are not enough for consistent results. Therefore, the results presented here have to be considered cautiously.\n",
        "\n",
        "We know from the first part of the question that the value function for the target policy is $$[0, -29.21,\\;  37.80,\\; 183.53 ,\\; 393.67 ,\\; 656.52,0].$$ \n",
        "Although the ranking of the states did not change, the magnitude of the values deviate a lot and we can deduct that our results are not accurate.\n"
      ]
    },
    {
      "metadata": {
        "id": "5pM10bnn0sfL",
        "colab_type": "code",
        "outputId": "a03b32ca-76e2-48d6-9c15-095d4a4952b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "cell_type": "code",
      "source": [
        "# run code again to show volatility of results\n",
        "run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Episode 0\n",
            "\n",
            " Episode 100000\n",
            "\n",
            " Episode 200000\n",
            "\n",
            " Episode 300000\n",
            "\n",
            " Episode 400000\n",
            "\n",
            " Q:\n",
            "[[  0.           0.        ]\n",
            " [ 38.93906364  30.04463831]\n",
            " [147.23555501 147.49445118]\n",
            " [324.23988473 287.80895261]\n",
            " [508.42581209 519.70388715]\n",
            " [767.90419338 757.61576977]\n",
            " [  0.           0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Oolg1EqMeDeY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "3. **[max points 10]** Compute the optimal policy by using the off-policy Monte Carlo algorithm for the behavior policy that in each state selects either action equiprobably. Show action values and policy. \n",
        "\n",
        "In P1-1, use as a termination criteria <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;||V_{t+1}-V_t||_{\\infty}{\\leq}1e-6\" title=\"x\" />. \n",
        "\n",
        "In P1-2 and P2-3, the number of episodes equal to `500,000`. "
      ]
    },
    {
      "metadata": {
        "id": "boDg-TGzOD3p",
        "colab_type": "code",
        "outputId": "3fb1f50c-ecef-45bc-dde6-a14f74c8d728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# parameter setting\n",
        "d = 3\n",
        "pH = 0.55\n",
        "pL = 0.45\n",
        "cH = 50\n",
        "cL = 10\n",
        "R = 1000\n",
        "num_episodes = 500000\n",
        "\n",
        "Q = np.reshape(np.random.uniform(size = 10),(5,2))\n",
        "C = np.zeros((5,2))\n",
        "pi = [np.argmax(Q[s,:]) for s in range(5)]\n",
        "\n",
        "\n",
        "for i in range(num_episodes):\n",
        "  if i%100000==0:\n",
        "    print(\"\\n Episode\", i)\n",
        "\n",
        "  # run once with behavior policy\n",
        "  states = []\n",
        "  rewards = []\n",
        "  actions = []\n",
        "  s = 0\n",
        "  while(abs(s) < 3):\n",
        "    states.append(s)\n",
        "    high_effort = (np.random.uniform() < 0.5) * 1\n",
        "    actions.append(high_effort)\n",
        "    p = high_effort * pH + (1 - high_effort) * pL\n",
        "    Xwins = (np.random.uniform() < p)\n",
        "    s = s + Xwins - (1 - Xwins)\n",
        "    c = high_effort * cH + (1 - high_effort) * cL\n",
        "    rewards.append(-c)\n",
        "  if s==3:\n",
        "    rewards[-1] += R\n",
        "\n",
        "  # update Q\n",
        "  G = 0\n",
        "  W = 1\n",
        "  for t in reversed(range(len(states))):\n",
        "    s = states[t]\n",
        "    a = actions[t]\n",
        "    G += rewards[t]\n",
        "    C[s+2,a] += W\n",
        "    Q[s+2,a] = Q[s+2,a] + W/C[s+2,a] * (G-Q[s+2,a])\n",
        "    pi = [np.argmax(Q[s_iter,:]) for s_iter in range(5)]\n",
        "    if a != pi[s+2]:\n",
        "      break\n",
        "    W /= 0.5\n",
        "        \n",
        "# print results\n",
        "print(\"\\n\")\n",
        "print(\"Q: \")\n",
        "print(np.vstack(([0,0],Q,[0,0])))\n",
        "print(\"\\n optimal policy: \")\n",
        "print(pi)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Episode 0\n",
            "\n",
            " Episode 100000\n",
            "\n",
            " Episode 200000\n",
            "\n",
            " Episode 300000\n",
            "\n",
            " Episode 400000\n",
            "\n",
            "\n",
            "Q: \n",
            "[[  0.           0.        ]\n",
            " [ 48.76846604  24.79422522]\n",
            " [198.56245155 160.44971454]\n",
            " [333.0351695  354.06297233]\n",
            " [557.99715599 532.30111576]\n",
            " [763.93321757 786.35133003]\n",
            " [  0.           0.        ]]\n",
            "\n",
            " optimal policy: \n",
            "[0, 0, 1, 0, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p23RDobf6umV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The optimal policy defines in its $i^{th}$ entry spending high effort in the state with score difference $i-2$ if that entry is 1 for $i\\in\\{0,..,4\\}$. Thus the optimal policy identified here is [low effort, low effort, low effort, high effort, low effort, high effort] for a score difference of [-2,-1,0,1,2]."
      ]
    },
    {
      "metadata": {
        "id": "I6Gv44cEeGUv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## P2 (max points 25)"
      ]
    },
    {
      "metadata": {
        "id": "ca_1WmlXeKOw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Consider the same reinforcement learning problem as in P1 but for the following questions:\n",
        "\n",
        "1. **[max points 5]** Solve the problem by using Q-learning algorithm. Use <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\epsilon\" title=\"x\" />-greedy with <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\epsilon=0.1\" title=\"x\" />. "
      ]
    },
    {
      "metadata": {
        "id": "4Jxng_5KGCAS",
        "colab_type": "code",
        "outputId": "da3a9dab-5bd2-4543-bb47-d0625d59380a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# parameter setting\n",
        "d = 3\n",
        "pH = 0.55\n",
        "pL = 0.45\n",
        "cH = 50\n",
        "cL = 10\n",
        "R = 1000\n",
        "num_episodes = 500000\n",
        "alpha = 0.1\n",
        "gamma = 1\n",
        "epsilon = 10**(-1)\n",
        "\n",
        "Q = np.reshape(np.concatenate([np.zeros(2),np.random.uniform(size = 10),np.zeros(2)]),(7,2))\n",
        "\n",
        "for i in range(num_episodes):\n",
        "    \n",
        "  if i%100000==0:\n",
        "    print(\"\\n Episode\", i)\n",
        "  \n",
        "  # initialization\n",
        "  s = 0\n",
        "  \n",
        "  # simulate game\n",
        "  while(abs(s) < 3):\n",
        "    # choose a with epsilon greedy\n",
        "    pi_a = np.argmax(Q[s+3,:])\n",
        "    explore = (np.random.uniform() < epsilon/2)\n",
        "    a = (1-explore) * pi_a + explore * (1-pi_a) # a=1 is high_effort\n",
        "    #  play one round\n",
        "    p = a * pH + (1 - a) * pL\n",
        "    Xwins = (np.random.uniform() < p)\n",
        "    s_new = s + Xwins - (1 - Xwins)\n",
        "    c = a * cH + (1 - a) * cL\n",
        "    r = - c\n",
        "    if s_new==3:\n",
        "      r += R\n",
        "    # update Q\n",
        "    Q[s+3,a] = Q[s+3,a] + alpha * (r + gamma*Q[s_new+3,:].max() - Q[s+3,a])\n",
        "    s = s_new\n",
        "\n",
        "# print results\n",
        "print(\"\\n\")\n",
        "print(\"Q: \")\n",
        "print(Q)\n",
        "pi = [np.argmax(Q[s_iter,:]) for s_iter in range(7)]\n",
        "print(\"\\n optimal policy: \")\n",
        "print(pi[1:6])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Episode 0\n",
            "\n",
            " Episode 100000\n",
            "\n",
            " Episode 200000\n",
            "\n",
            " Episode 300000\n",
            "\n",
            " Episode 400000\n",
            "\n",
            "\n",
            "Q: \n",
            "[[  0.           0.        ]\n",
            " [ 25.80011743  -8.88511303]\n",
            " [ 90.49958978  64.48008688]\n",
            " [236.3001801  158.10111405]\n",
            " [338.76240751 389.14884036]\n",
            " [640.22447451 557.8430969 ]\n",
            " [  0.           0.        ]]\n",
            "\n",
            " optimal policy: \n",
            "[0, 0, 0, 1, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pXQOws3vIEo1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The optimal policy here is to spend high effort for a score difference of 1 and low effort otherwise."
      ]
    },
    {
      "metadata": {
        "id": "JXSx0cGUeM6H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2. **[max points 5]** Solve the problem by using SARSA algorithm. Use <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\epsilon\" title=\"x\" />-greedy with  <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\epsilon=0.1\" title=\"x\" />."
      ]
    },
    {
      "metadata": {
        "id": "LqyHa7mTflQb",
        "colab_type": "code",
        "outputId": "36e47a44-10b9-498b-c964-b2dbd4c0ca2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# parameter setting\n",
        "d = 3\n",
        "pH = 0.55\n",
        "pL = 0.45\n",
        "cH = 50\n",
        "cL = 10\n",
        "R = 1000\n",
        "num_episodes = 500000\n",
        "alpha = 0.1\n",
        "epsilon = 10**(-1)\n",
        "gamma = 1\n",
        "\n",
        "Q = np.reshape(np.concatenate([np.zeros(2),np.random.uniform(size = 10),np.zeros(2)]),(7,2))\n",
        "\n",
        "for i in range(num_episodes):\n",
        "  if i%100000==0:\n",
        "    print(\"\\n Episode\", i)\n",
        "  \n",
        "  # initialization\n",
        "  s = 0\n",
        "  pi_a = np.argmax(Q[s+3,:])\n",
        "  explore = (np.random.uniform() < epsilon/2)\n",
        "  a = (1-explore) * pi_a + explore * (1-pi_a) # a=1 is high_effort\n",
        "  \n",
        "  # simulate game\n",
        "  while(abs(s) < 3):\n",
        "    # take action a    \n",
        "    p = a * pH + (1 - a) * pL\n",
        "    Xwins = (np.random.uniform() < p)\n",
        "    s_new = s + Xwins - (1 - Xwins)\n",
        "    c = a * cH + (1 - a) * cL\n",
        "    r = - c\n",
        "    if s_new==3:\n",
        "      r += R\n",
        "    # epsilon greedy\n",
        "    pi_a = np.argmax(Q[s_new+3,:])\n",
        "    explore = (np.random.uniform() < epsilon/2)\n",
        "    a_new = (1-explore) * pi_a + explore * (1-pi_a) # a=1 is high_effort\n",
        "    \n",
        "    # update Q\n",
        "    Q[s+3,a] = Q[s+3,a] + alpha * (r + gamma*Q[s_new+3,a_new] - Q[s+3,a])\n",
        "    \n",
        "    s = s_new\n",
        "    a = a_new\n",
        "    \n",
        "# print results\n",
        "print(\"\\n\")\n",
        "print(\"Q: \")\n",
        "print(Q)\n",
        "pi = [np.argmax(Q[s_iter,:]) for s_iter in range(7)]\n",
        "print(\"\\n optimal policy: \")\n",
        "print(pi[1:6])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Episode 0\n",
            "\n",
            " Episode 100000\n",
            "\n",
            " Episode 200000\n",
            "\n",
            " Episode 300000\n",
            "\n",
            " Episode 400000\n",
            "\n",
            "\n",
            "Q: \n",
            "[[  0.           0.        ]\n",
            " [ 34.1840996   10.35144786]\n",
            " [131.7287523   29.10910677]\n",
            " [255.41490083 200.42594894]\n",
            " [388.37891304 349.87724244]\n",
            " [675.45298399 685.17521241]\n",
            " [  0.           0.        ]]\n",
            "\n",
            " optimal policy: \n",
            "[0, 0, 0, 0, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NdgGQOVBeO2R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "3. **[max points 15]** Assume that the agent follows a random policy. Evaluate the problem by using on-line TD(<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\lambda\" title=\"x\" />)-algorithm. Use the values of parameters <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\lambda=0.9\" title=\"x\" /> and step size <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\eta=0.0001\" title=\"x\" />.\n",
        "\n",
        "In all questions, use the number of episodes equal to `500,000`. For each question report the estimated values and policy. "
      ]
    },
    {
      "metadata": {
        "id": "y8rPDfMfoS6m",
        "colab_type": "code",
        "outputId": "9c79c1fc-3c41-4bac-cfaf-6421bed2c9bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# parameter setting\n",
        "d = 3\n",
        "pH = 0.55\n",
        "pL = 0.45\n",
        "cH = 50\n",
        "cL = 10\n",
        "R = 1000\n",
        "num_episodes = 500000\n",
        "lambd = 0.9\n",
        "eta = 10**(-4)\n",
        "gamma = 1\n",
        "# Initialization\n",
        "v = np.concatenate([np.zeros(1),np.random.uniform(size = 5),np.zeros(1)])\n",
        "e = np.zeros(7)\n",
        "\n",
        "for i in range(num_episodes):\n",
        "    \n",
        "  if i%100000==0:\n",
        "    print(\"\\n Episode\", i)\n",
        "  \n",
        "  s = 0\n",
        "  \n",
        "  while(abs(s) < 3):\n",
        "    # choose a from random policy\n",
        "    a = (np.random.uniform() < 0.5)\n",
        "    # play a round  of the game\n",
        "    p = a * pH + (1 - a) * pL\n",
        "    Xwins = (np.random.uniform() < p)\n",
        "    s_new = s + Xwins - (1 - Xwins)\n",
        "    c = a * cH + (1 - a) * cL\n",
        "    r = - c\n",
        "    if s_new==3:\n",
        "      r += R\n",
        "    delta = r + gamma*v[s_new+3] - v[s+3]\n",
        "    e[s+3] += 1\n",
        "    for s_iter in range(7):\n",
        "      v[s_iter] += eta*e[s_iter]*delta\n",
        "      e[s_iter] *= lambd*gamma\n",
        "    s = s_new\n",
        "    \n",
        "print(\"\\n\")\n",
        "print(\"V: \", [round(v[i],2) for i in range(7)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Episode 0\n",
            "\n",
            " Episode 100000\n",
            "\n",
            " Episode 200000\n",
            "\n",
            " Episode 300000\n",
            "\n",
            " Episode 400000\n",
            "\n",
            "\n",
            "V:  [0.0, 16.61, 90.13, 218.39, 416.89, 678.45, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b6YSOgoleSOS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## P3 (max points 25)\n",
        "\n",
        "Consider a variant of the reinforcement learning problem which is identical to that in P1 but where the player does not incur a cost by \n",
        "investing effort but can invest high effort if she has a sufficient energy. \n",
        "\n",
        "The player starts with a given energy level <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;B\" title=\"x\" /> which is decremented by <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;c_H\" title=\"x\" />whenever in a round the player invests high effort. \n",
        "If the energy level would become negative after subtracting the value of <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;c_H\" title=\"x\" />, it is set equal to <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;0\" title=\"x\" />. \n",
        "In each round, an amount of energy of value <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;a\" title=\"x\" /> is added to the player, independently with probability <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;p\" title=\"x\" />. \n",
        "\n",
        "The player can invest high effort in a round only if her energy level is at least <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;c_H\" title=\"x\" />.\n",
        "\n",
        "The player receives a prize of value <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;R\" title=\"x\" /> if winning the game and this is the only reward received.\n",
        "\n",
        "Use the following setting of parameters:\n",
        "* <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;p_H=0.55\" title=\"x\" />\n",
        "* <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;p_L=0.45\" title=\"x\" />\n",
        "* <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;c_H=1\" title=\"x\" />\n",
        "* <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;R=1000\" title=\"x\" />\n",
        "* <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;B=10\" title=\"x\" />\n",
        "* <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;a=2\" title=\"x\" />\n",
        "* <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;p=0.2\" title=\"x\" />\n",
        "* step size <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\eta=0.001\" title=\"x\" />\n",
        "* Discount parameter <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\gamma=0.9\" title=\"x\" />\n",
        "\n",
        "1. Solve this problem by using SARSA(<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\lambda\" title=\"x\" />) algorithm, for the value of parameter <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\lambda=0.9\" title=\"x\" />. Use `500,000` episodes. \n",
        "Show the estimated action values for different states and policy. Discuss the results. \n"
      ]
    },
    {
      "metadata": {
        "id": "qawbRqpfLMtz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# parameter setting\n",
        "d = 3\n",
        "pH = 0.55\n",
        "pL = 0.45\n",
        "cH = 1\n",
        "R = 1000\n",
        "B = 10\n",
        "a_B = 2\n",
        "p_B = 0.2\n",
        "B_max = 10\n",
        "num_episodes = 500000\n",
        "lambd = 0.9\n",
        "gamma = 0.9\n",
        "eta = 10**(-3)\n",
        "epsilon = 10**(-1)\n",
        "\n",
        "# initialization\n",
        "Q = np.zeros((11,7,2))\n",
        "Q[:,range(1,6),:] = np.reshape(np.random.uniform(low=0,high=800,size=110), ((11,5,2)))\n",
        "Q[:,range(1,6),1] = np.zeros(5)\n",
        "e = np.zeros((11,7,2))\n",
        "\n",
        "for i in range(num_episodes):\n",
        "  np.random.seed(i)\n",
        "  if i%50000==0:\n",
        "    print(\"\\n Episode\", i)\n",
        "  \n",
        "  # simulate the game\n",
        "  s = 0\n",
        "  B = 10\n",
        "  a = np.argmax(Q[B,s+3,:])\n",
        "  r = 0\n",
        "  \n",
        "  while(abs(s) < 3):\n",
        "    # take action a and observe\n",
        "    B_new = min(B - a*cH + (np.random.uniform() < p_B)*a_B, B_max)\n",
        "    p_win = a * pH + (1 - a) * pL\n",
        "    Xwins = (np.random.uniform() < p_win)\n",
        "    s_new = s + Xwins - (1 - Xwins)\n",
        "    if s_new==3:\n",
        "      r = R\n",
        "    # choose a acc to epsilon greedy. a=0 if not enough energy.\n",
        "    if B_new >= cH:\n",
        "      explore = (np.random.uniform() < epsilon/2)\n",
        "      a_wo_exp = np.argmax(Q[B_new,s_new+3,:])\n",
        "      a_new = (1-explore) * a_wo_exp + explore * (1-a_wo_exp) # a=1 is high_effort\n",
        "    else:\n",
        "      a_new = 0\n",
        "    delta = r + gamma*Q[B_new,s_new+3,a_new] - Q[B,s+3,a]\n",
        "    e[B,s+3,a] += 1\n",
        "    for  B_iter in range(11):\n",
        "      for s_iter in range(1,6):\n",
        "        for a_iter in range(2):\n",
        "          Q[B_iter,s_iter,a_iter] += eta*e[B_iter,s_iter,a_iter]*delta\n",
        "          e[B_iter,s_iter,a_iter] *= gamma*lambd\n",
        "    s = s_new\n",
        "    a = a_new\n",
        "    B = B_new\n",
        "    \n",
        "pi = np.array([[np.argmax(Q[B_iter,s_iter,:]) for s_iter in range(7)] for B_iter in range(11)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IIW-firfJuIt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "'Q' has  state action values. First dimension is energy level 'B', second dimension the score difference 's' as defined in P1. Third dimension is action. Action 0 means spend low effort, 1 means high effort. States are here a combination of  's' and 'B'. Note that 'Q' is thus actually two dimensional. For readability we print it as three-dimensional matrix here.  \n",
        "\n",
        "Note that the values for Q[0, :,  1] will not be updated in the  algorithm since it is not possible to act with high effort if B=0."
      ]
    },
    {
      "metadata": {
        "id": "SBhVcEt79QgV",
        "colab_type": "code",
        "outputId": "f82ae46a-9dcf-4c16-eb22-231669420b45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1568
        }
      },
      "cell_type": "code",
      "source": [
        "Q"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[  0.        ,   0.        ],\n",
              "        [105.31793684,   0.        ],\n",
              "        [120.94012473,   0.        ],\n",
              "        [222.72242041,   0.        ],\n",
              "        [677.8335695 ,   0.        ],\n",
              "        [288.47208386,   0.        ],\n",
              "        [  0.        ,   0.        ]],\n",
              "\n",
              "       [[  0.        ,   0.        ],\n",
              "        [212.64516788,   2.94253343],\n",
              "        [202.41042098,   6.40334545],\n",
              "        [369.17653735,  14.91184465],\n",
              "        [539.44516513,  15.72431204],\n",
              "        [412.04145775,  15.01359505],\n",
              "        [  0.        ,   0.        ]],\n",
              "\n",
              "       [[  0.        ,   0.        ],\n",
              "        [ 44.19957814,  22.71712998],\n",
              "        [109.91222502,  51.48011307],\n",
              "        [197.75265041, 103.8372161 ],\n",
              "        [338.52896121,  91.03689354],\n",
              "        [593.2393755 ,  88.87568045],\n",
              "        [  0.        ,   0.        ]],\n",
              "\n",
              "       [[  0.        ,   0.        ],\n",
              "        [ 55.25172249,  33.76736124],\n",
              "        [115.09646483, 122.01870847],\n",
              "        [211.92900899, 170.88480488],\n",
              "        [361.27338449, 343.92699571],\n",
              "        [632.14575827, 318.99471496],\n",
              "        [  0.        ,   0.        ]],\n",
              "\n",
              "       [[  0.        ,   0.        ],\n",
              "        [ 50.0087538 ,  60.10763382],\n",
              "        [132.73447709, 116.88323427],\n",
              "        [230.17501679, 241.504274  ],\n",
              "        [388.82758995, 343.90972862],\n",
              "        [647.7853306 , 601.96826498],\n",
              "        [  0.        ,   0.        ]],\n",
              "\n",
              "       [[  0.        ,   0.        ],\n",
              "        [ 69.86510331,  39.10064784],\n",
              "        [125.96777832, 153.64080068],\n",
              "        [245.66359136, 216.70683273],\n",
              "        [402.28487408, 418.2165017 ],\n",
              "        [674.47148172, 474.54281738],\n",
              "        [  0.        ,   0.        ]],\n",
              "\n",
              "       [[  0.        ,   0.        ],\n",
              "        [ 55.90446924,  79.06151162],\n",
              "        [147.00205285, 124.79973005],\n",
              "        [244.30414092, 275.14698159],\n",
              "        [436.82952373, 402.37769645],\n",
              "        [661.59494443, 716.72371991],\n",
              "        [  0.        ,   0.        ]],\n",
              "\n",
              "       [[  0.        ,   0.        ],\n",
              "        [ 71.7604358 ,  45.4448936 ],\n",
              "        [151.91519521, 170.79762437],\n",
              "        [292.12785805, 251.40476992],\n",
              "        [421.96059056, 469.83993901],\n",
              "        [673.91674521, 652.04788134],\n",
              "        [  0.        ,   0.        ]],\n",
              "\n",
              "       [[  0.        ,   0.        ],\n",
              "        [ 60.83178649,  90.761254  ],\n",
              "        [154.83104889, 162.4845224 ],\n",
              "        [275.6693691 , 298.93174141],\n",
              "        [434.76165802, 454.91989183],\n",
              "        [687.75308321, 744.37788971],\n",
              "        [  0.        ,   0.        ]],\n",
              "\n",
              "       [[  0.        ,   0.        ],\n",
              "        [ 71.1778634 ,  89.90696525],\n",
              "        [163.23884307, 188.15664278],\n",
              "        [280.60672375, 280.14005502],\n",
              "        [442.55642548, 486.44580327],\n",
              "        [679.24096376, 736.83134412],\n",
              "        [  0.        ,   0.        ]],\n",
              "\n",
              "       [[  0.        ,   0.        ],\n",
              "        [ 67.19786273,  92.03306222],\n",
              "        [169.51566295, 176.28839313],\n",
              "        [279.5741225 , 305.83499237],\n",
              "        [453.87637383, 481.22009744],\n",
              "        [682.76063534, 759.06157469],\n",
              "        [  0.        ,   0.        ]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "NoEd06aVeeu1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Again our optimal policy is dependent on the state which depends on the score difference and the energy level. In  the following we display the optimal action (1 for high effort, 0 for low effort) for energy level B and score difference s in row B and column s+2."
      ]
    },
    {
      "metadata": {
        "id": "G9g_vLXEBfog",
        "colab_type": "code",
        "outputId": "c0cf506d-9431-404c-a58b-da5207b61cb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "cell_type": "code",
      "source": [
        "pi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0],\n",
              "       [1, 0, 1, 0, 0],\n",
              "       [0, 1, 0, 1, 0],\n",
              "       [1, 0, 1, 0, 1],\n",
              "       [0, 1, 0, 1, 0],\n",
              "       [1, 1, 1, 1, 1],\n",
              "       [1, 1, 0, 1, 1],\n",
              "       [1, 1, 1, 1, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "CgjW3QS3LMvA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The resulting policy favors putting in high effort when B is high, which is reasonable since  there is no costs incurred in spending energy such that it would be optimal to have spent all the energy you have after each game. For low values of B, the energy is saved and thus actions of high  effort are less favorable then.\n",
        "\n",
        "However, note that 500,000 iterations is not enough in order to achieve consistent results for 77 states. Especially those states with low energy level are not incurred that often such that their value has high volatility."
      ]
    },
    {
      "metadata": {
        "id": "Be6CdU2OeYU5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## P4 (max points 25)\n",
        "\n",
        "Consider the following two-player game of chance. \n",
        "Two players, we refer to as players <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;X\" title=\"x\" /> and <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;Y\" title=\"x\" />, have initial endowments of <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;x\" title=\"x\" /> and <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;y\" title=\"x\" /> tokens, respectively. \n",
        "Assume that <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;x,y>0\" title=\"x\" /> . The game proceeds over rounds where in each round a dice is rolled. \n",
        "If the outcome of the dice is <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;1,2\" title=\"x\" /> or <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;3\" title=\"x\" />, player <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;Y\" title=\"x\" /> loses one token, and otherwise, \n",
        "if the outcome is <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;4,5\" title=\"x\" /> or <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;6\" title=\"x\" />, player <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;X\" title=\"x\" /> loses one token. \n",
        "The game ends as soon one of the players runs out of tokens. \n",
        "The winner of the game is the player who at the end of the game has at least one token left.\n",
        "\n",
        "Answer the following questions:\n",
        "\n",
        "1. **[max points 5]** What is the winning probability of player <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;X\" title=\"x\" /> for <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;x=1\" title=\"x\" /> and each value of <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;y\" title=\"x\" />?"
      ]
    },
    {
      "metadata": {
        "id": "ZOMc5uvT2TuN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The probability of $X$ winning given $x=1$ is $0.5^y$.\n",
        "\n",
        "Proof by induction over $y$. \n",
        "\n",
        "Base case $y=1$: Probability of winning for player $X$ is $3/6=0.5$.\n",
        "\n",
        "Inductive step: We assume that $0.5^k$ is the probability of winning for player $X$ for some $k\\in\\mathbb{N}$ such that $y=k$ (induction hypothesis).<br/>\n",
        "Now assume $y=k+1$. This means that player $Y$ has $k+1$ tokens.\n",
        "Player $X$ wins with a probability of $0.5$ in the first round. Player $Y$ thus loses a token and has only $k$ tokens left. Now the probability of $X$ winning the whole game is $0.5^k$ according to our induction hypothesis.If $X$ loses the first round, he has no tokens left and the probability of winning becomes 0. <br/>\n",
        "So the probability of winning if $Y$ has $k+1$ tokens is $0.5^k\\cdot 0.5=0.5^{k+1}$."
      ]
    },
    {
      "metadata": {
        "id": "0YmC_h-recmT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2. **[max points 5]** What is the winning probability of player <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;X\" title=\"x\" /> for <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;y=1\" title=\"x\" /> and each value of <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;x\" title=\"x\" />?"
      ]
    },
    {
      "metadata": {
        "id": "TYbAPMJo7MlY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The probability of $X$ winning with $x$ tokens given $Y$ has $y=1$ token is the same as the probability of $Y$ winning given $X$ has 1 token and $Y$ has $x$ tokens. This is the complement of the event asked for in the first question if $y$ is switched with $x$. Thus the probability searched for here is $1-0.5^x$."
      ]
    },
    {
      "metadata": {
        "id": "xLQ9Ljzweexd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "3. **[max points 15]** What is the winning probability of player <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;X\" title=\"x\" /> for each value of <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;x\" title=\"x\" /> and <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;y\" title=\"x\" />?\n",
        "\n",
        "**Note:** You need to show derivations for your solutions.  "
      ]
    },
    {
      "metadata": {
        "id": "gIaKv8-g797B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The winning probability of player $X$ in this case is \n",
        "$$0.5^y\\left(\\sum_{k=0}^{x-1}0.5^k \\binom{y+k-1}{k}\\right).$$\n",
        "\n",
        "Assume player $X$ has $x$ tokens and player $Y$ has $y$ tokens.\n",
        "Player $X$ wins the whole game if and only if he wins $y$ times (since $Y$ has to lose all his tokens) and loses not more than $x-1$ tokens (since $X$ has to retain at least 1 token in order to win). Let $k$ denote the number of tokens $X$ loses. Then the game will finish after $y+k$ dice rolls since $Y$ has to lose $y$ times and $X$ loses $k$ times. For $X$ to finish the game after $y-k$ rounds, the last round has to be a win for $X$. The order of the other won and lost rounds does not matter.  The $k$ tokens have to be lost in the first $y+k-1$ rounds. There are hence $\\binom{y+k-1}{k}$ possibilites of distributing the $k$ losses across the $y+k$ matches. The probability that $X$ loses $k$ tokens and wins $y$ rounds in a specific order is $0.5^k\\cdot0.5^y$. All in all we have \n",
        "$$\\sum_{k=0}^{x-1} 0.5^y0.5^k \\binom{y+k-1}{k}.$$\n"
      ]
    }
  ]
}